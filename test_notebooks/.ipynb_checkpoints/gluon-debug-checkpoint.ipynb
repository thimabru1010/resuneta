{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug deep learning program\n",
    "\n",
    "In this tutorial we'll walk through some common issues during deep learning application development and methods to resolve.\n",
    "\n",
    "We pick handwritten digit recognition application with multilayer perceptron as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn, data\n",
    "from mxnet import autograd as ag\n",
    "\n",
    "class Net(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.conv1 = nn.Conv2D(20, kernel_size=(5,5))\n",
    "            self.pool1 = nn.MaxPool2D(pool_size=(2,2), strides = (2,2))\n",
    "            self.conv2 = nn.Conv2D(50, kernel_size=(5,5))\n",
    "            self.pool2 = nn.MaxPool2D(pool_size=(2,2), strides = (2,2))\n",
    "            self.fc1 = nn.Dense(500)\n",
    "            self.fc2 = nn.Dense(10)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.pool1(F.tanh(self.conv1(x)))\n",
    "        print(\"pool1 output: %s\" % str(x))\n",
    "        x = self.pool2(F.tanh(self.conv2(x)))\n",
    "        x = x.reshape((0, -1))\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Check Data IO Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use standard dataset\n",
    "Standard datasets, such as mnist and cifar10, are ideal starting points and can help with minimizing the issues in input data itself.\n",
    "In this tutorial we use mnist as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = mx.test_utils.get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check data loader\n",
    "MXNet gluon uses DataLoader class for data io. We need to create a Dataset object which wraps up input data and a Sampler object which defines how to draw data samples. Gluon has some built-in classes for most common use cases. In this tutorial we use built-in ArrayDataset and BatchSampler. If you need to implement customized Dataset or Sampler, add some unit tests to ensure these customized data loading modules behave as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dataset = data.dataset.ArrayDataset(mnist['train_data'], mnist['train_label'])\n",
    "val_dataset = data.dataset.ArrayDataset(mnist['test_data'], mnist['test_label'])\n",
    "train_dataloader = data.dataloader.DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = data.dataloader.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check data preprocessing\n",
    "Input data usually requires preprocessing and to be standardized. In this tutorial, the pixels in input mnist images are divied by 255 and limited between 0 and 1.0:\n",
    "```\n",
    "   image = image.reshape(image.shape[0], 1, 28, 28).astype(np.float32)/255\n",
    "```\n",
    "Some networks require input pixels to between -1.0 and 1.0. Don't forget to preprocess input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Check Implementation Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check the correctness of loss function\n",
    "We use SoftmaxCrossEntropyLoss as loss function. Similar to data loader, you can create customized loss function. Add unit tests to ensure the correctness.\n",
    "A common issue in implementing customized loss function is numerical instability. Many loss functions use logistic function and we need to make sure the input shouldn't be small enough to return 'nan'. Cilpping input is a common way to deal such situation:\n",
    "```\n",
    "    eps = 10e-8\n",
    "    input = mx.nd.clip(input, a_min=eps, a_max=1.0 - eps)\n",
    "    output = mx.nd.log(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to set from_logits=True if input is already a log probability:\n",
    "```\n",
    "    loss_fuc = gluon.loss.SoftmaxCrossEntropyLoss(from_logits=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Check parameter initialization\n",
    "If your are not sure, Xavier is a good choice to start. Try different initializers if current initialization leads your model to a bad local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "ctx = [mx.cpu()]\n",
    "model.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check trainer\n",
    "Gluon defines optimizer in Trainer module. You need to setup the parameters to be updated. Call collect_params() to specify the model paramters. Make sure you collect the parameters for the correct model. \n",
    "\n",
    "Also try different hyperparameters or select different optimizers if the training can't get much progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Debug during training\n",
    "\n",
    "One big advantage of gluon is that you can easily switch between imperative and symbolic training. Imperative mode is suitable for debug. You can add print statements in forward function. While debugging is finished, you can call hybridize() to use hybrid mode to accelerate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Watch layer output\n",
    "Insert print statements to forward function to monitor layer outputs. In this tutorial, we print the ouputs of pool1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 1\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    if i >= iter_num:\n",
    "        break\n",
    "    output = model(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Watch parameters values\n",
    "We can also print parameter values. Call block.params to get ParameterDict of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_params = model.fc1.params\n",
    "print(fc1_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print values of fc1 layer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in fc1_params.items():\n",
    "    print(\"%s:\" % key)\n",
    "    print(str(val.data()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Watch gradients\n",
    "We can also print gradients to see if gradients vanish or explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 1\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    if i >= iter_num:\n",
    "        break\n",
    "    data = batch[0]\n",
    "    label = batch[1]\n",
    "    with ag.record():\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, label)\n",
    "        loss.backward() \n",
    "\n",
    "for key, val in fc1_params.items():\n",
    "    print(\"%s grad:\" % key)\n",
    "    print(str(val.grad()) + '\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "display_name": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": ""
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
